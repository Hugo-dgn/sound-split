{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"xBvdio9IE1TB"},"source":["## Import des libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"aFOfNAFFE1TG"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/hugo/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import os\n","from scipy.io import wavfile\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","import torchinfo"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"8DDcCG-BE1TH"},"source":["## Téléchargement du dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRl7xvE1E1TH"},"outputs":[],"source":["!gdown 1Q3sbordKpx65ExOvqtM4fif7rDXhIXwS\n","!unzip datasetaudio.zip"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"eJxJ2WZtE1TI"},"source":["Sinon, le lien est ici: https://drive.google.com/file/d/1Q3sbordKpx65ExOvqtM4fif7rDXhIXwS/view?usp=sharing"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"cPyxcartE1TI"},"source":["## Load le dataset\n","\n","### Load le dataset de train\n","Pour load un fichier .wav, on utilise la fonction `wavfile.read()` de `scipy.io` qui nous retourne un tuple `(samplerate, data)`. `data` est un array numpy de dimension 1, contenant les valeurs du signal audio. Le nombre d'échantillons par seconde pour nos données est de 4000, donc chaque fichier .wav contient 6000 valeurs car la durée du signal est de 1.5 secondes. `data` est donc de shape `(6000,)`.\n","\n","Il y a 4096 fichiers .wav dans le dossier `train/x_train` et 2048 fichiers .wav dans le dossier `test/x_test`. Donc on peut créer un array numpy de shape `(4096, 6000)` pour les données d'entraînement et un array numpy de shape `(2048, 6000)` pour les données de test."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3Yk5YxmE1TI","outputId":"4b8c6bef-e4b4-4f93-aa36-ac7a5a037661"},"outputs":[{"name":"stdout","output_type":"stream","text":["(4096, 6000)\n","(4096, 6000)\n","(4096, 6000)\n"]}],"source":["X_train = []\n","Y1_train = []\n","Y2_train = []\n","\n","N_TRAIN = 4096\n","\n","for i in range(N_TRAIN):\n","    x = wavfile.read(f\"train/x_train/{i}.wav\")[1]\n","    y1 = wavfile.read(f\"train/y_train/{i}-a.wav\")[1]\n","    y2 = wavfile.read(f\"train/y_train/{i}-b.wav\")[1]\n","\n","    X_train.append(x)\n","    Y1_train.append(y1)\n","    Y2_train.append(y2)\n","\n","X_train = np.array(X_train)\n","Y1_train = np.array(Y1_train)\n","Y2_train = np.array(Y2_train)\n","\n","print(X_train.shape)\n","print(Y1_train.shape)\n","print(Y2_train.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Data augmentation"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(8192, 6000)\n","(8192, 6000)\n","(8192, 6000)\n"]}],"source":["fact = 2**1\n","aug = (fact-1)*N_TRAIN\n","\n","#get aug pairs of random indices\n","idx = np.random.randint(0, N_TRAIN, size=(aug, 2))\n","#sum the signals\n","Y1_train_aug = Y1_train[idx[:,0]]\n","Y2_train_aug = Y2_train[idx[:,1]]\n","X_train_aug = Y1_train_aug + Y2_train_aug\n","X_train_aug.shape\n","\n","X_train = np.concatenate([X_train, X_train_aug])\n","Y1_train = np.concatenate([Y1_train, Y1_train_aug])\n","Y2_train = np.concatenate([Y2_train, Y2_train_aug])\n","\n","print(X_train.shape)\n","print(Y1_train.shape)\n","print(Y2_train.shape)\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"xUSBhIGRE1TJ"},"source":["### Load le dataset de test"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aV-kx9CJE1TK","outputId":"f9227049-c632-4b07-8e49-c73c7fba3aa4"},"outputs":[{"name":"stdout","output_type":"stream","text":["(512, 6000)\n"]}],"source":["X_test = []\n","\n","N_TEST = 512\n","\n","for i in range(N_TEST):\n","    x = wavfile.read(f\"test/x_test/{i}.wav\")[1]\n","    X_test.append(x)\n","\n","X_test = np.array(X_test)\n","\n","print(X_test.shape)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"clNGtVtnE1TK"},"source":["## Lecture des données"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MyHkXzuIE1TK"},"outputs":[],"source":["import IPython.display as ipd\n","\n","SAMPLERATE = 4000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"YQI0VeROE1TK","outputId":"ebff1e2b-dc7a-46c1-f7c1-4e23db3d68ed"},"outputs":[],"source":["\n","ipd.Audio(X_train[0], rate=SAMPLERATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"d-w8hR-ME1TL","outputId":"f39bc228-fec4-4500-e334-84e8c6622bf1"},"outputs":[],"source":["ipd.Audio(Y1_train[0], rate=SAMPLERATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"WPstIGhnE1TL","outputId":"31448f45-cb5b-4a1d-8d6c-bc6043ebaa4b"},"outputs":[],"source":["ipd.Audio(Y2_train[0], rate=SAMPLERATE)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"GeEixPgNE1TM"},"source":["## Batch les données"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQI0c5SuE1TM","outputId":"8e98889e-963b-487c-d189-e6bed58dea93"},"outputs":[{"name":"stdout","output_type":"stream","text":["(128, 64, 6000)\n","(128, 64, 6000)\n","(128, 64, 6000)\n"]}],"source":["batch_size = 64\n","X_train_reshaped = X_train.reshape(-1, batch_size, 6000)\n","Y1_train_reshaped = Y1_train.reshape(-1, batch_size, 6000)\n","Y2_train_reshaped = Y2_train.reshape(-1, batch_size, 6000)\n","\n","print(X_train_reshaped.shape)\n","print(Y1_train_reshaped.shape)\n","print(Y2_train_reshaped.shape)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"XnAElbtRE1TM"},"source":["## Convertir les données en torch.tensor"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"9bdbjUkWE1TN"},"outputs":[],"source":["X_train_torch = torch.from_numpy(X_train_reshaped).float()\n","Y1_train_torch = torch.from_numpy(Y1_train_reshaped).float()\n","Y2_train_torch = torch.from_numpy(Y2_train_reshaped).float()"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"1FNjFObBE1TN"},"source":["## Faire la même chose pour le dataset de test"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HynKreAE1TN","outputId":"6ff3b654-6b7d-4f6c-a79d-182b50d89489"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 64, 6000])\n"]}],"source":["X_test_reshaped = X_test.reshape(-1, batch_size, 6000)\n","X_test_torch = torch.from_numpy(X_test_reshaped).float()\n","\n","print(X_test_torch.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Netoyage"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#delete unsuse arrays\n","del X_train\n","del Y1_train\n","del Y2_train\n","del X_test\n","del X_train_reshaped\n","del Y1_train_reshaped\n","del Y2_train_reshaped\n","del X_test_reshaped"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"dditrgIFE1TO"},"source":["## Créer le modèle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHo_cqBxE1TO"},"outputs":[],"source":["kernel_size = 51\n","\n","class Conv1D(nn.Module):\n","    def __init__(self):\n","        super(Conv1D, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(32),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(64),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(128),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(256),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(512),\n","            nn.PReLU()\n","        )\n","        \n","        self.decoder = nn.Sequential(\n","            nn.Conv1d(in_channels=512, out_channels=256, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(256),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=256, out_channels=128, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(128),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(64),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=64, out_channels=32, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(32),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=32, out_channels=1, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","        )\n","        \n","    def forward(self, x):\n","        x = x.unsqueeze(1)\n","        x1 = self.encoder(x)\n","        x1 = self.decoder(x1)\n","        x2 = x - x1\n","        \n","        return x1.squeeze(1), x2.squeeze(1)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class conv_block(nn.Module):\n","    def __init__(self, in_c, out_c, kernel_size):\n","        super().__init__()\n","        self.network = nn.Sequential(\n","            nn.Conv1d(in_c, out_c, kernel_size=kernel_size, padding=\"same\"),\n","            nn.BatchNorm1d(out_c),\n","            nn.ReLU(),\n","            nn.Conv1d(out_c, out_c, kernel_size=kernel_size, padding=\"same\"),\n","            nn.BatchNorm1d(out_c),\n","            nn.ReLU()\n","        )\n","    def forward(self, x):\n","        y = self.network(x)\n","        return y\n","\n","class encoder_block(nn.Module):\n","    def __init__(self, in_c, out_c, kernel_size):\n","        super().__init__()\n","        self.conv = conv_block(in_c, out_c, kernel_size)\n","        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n","    def forward(self, x):\n","        y = self.conv(x)\n","        p = self.pool(y)\n","        return x, p\n","\n","class decoder_block(nn.Module):\n","    def __init__(self, in_c, out_c, in_skip, kernel_size):\n","        super().__init__()\n","        self.up = nn.ConvTranspose1d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n","        self.conv = conv_block(out_c+in_skip, out_c, kernel_size)\n","    def forward(self, inputs, skip):\n","        x = self.up(inputs)\n","        x = torch.cat([x, skip], axis=1)\n","        x = self.conv(x)\n","        return x\n","\n","class Unet(nn.Module):\n","    def __init__(self, kernel_size):\n","        super().__init__()\n","        self.e1 = encoder_block(1, 16, kernel_size=kernel_size)\n","        self.e2 = encoder_block(16, 32, kernel_size=kernel_size)\n","        self.e3 = encoder_block(32, 64, kernel_size=kernel_size)\n","        self.e4 = encoder_block(64, 64, kernel_size=kernel_size)\n","        self.b = conv_block(64, 128, kernel_size=kernel_size)\n","        self.d1 = decoder_block(128, 64, 64, kernel_size=kernel_size)\n","        self.d2 = decoder_block(64, 32, 32, kernel_size=kernel_size)\n","        self.d3 = decoder_block(32, 16, 16, kernel_size=kernel_size)\n","        self.d4 = decoder_block(16, 8, 1, kernel_size=kernel_size)\n","        self.outputs = nn.Conv1d(8, 1, kernel_size=1, padding=0)\n","        \n","    def forward(self, inputs):\n","        inputs = inputs.unsqueeze(1)\n","        s1, p1 = self.e1(inputs)\n","        s2, p2 = self.e2(p1)\n","        s3, p3 = self.e3(p2)\n","        s4, p4 = self.e4(p3)\n","        b = self.b(p4)\n","        d1 = self.d1(b, s4)\n","        d2 = self.d2(d1, s3)\n","        d3 = self.d3(d2, s2)\n","        d4 = self.d4(d3, s1)\n","        outputs = self.outputs(d4)\n","        \n","        y1 = outputs\n","        y2 = inputs - outputs\n","        return y1.squeeze(1), y2.squeeze(1)\n","\n","class MultiUnet(nn.Module):\n","    def __init__(self, list_kernel_size):\n","        super().__init__()\n","        self.len = len(list_kernel_size)\n","        for i, kernel_size in enumerate(list_kernel_size):\n","            setattr(self, f\"unet{i}\", Unet(kernel_size))\n","            \n","        self.fuse = nn.Conv1d(self.len, 1, kernel_size=1, padding=0)\n","        \n","    def forward(self, inputs):\n","        Y1 = []\n","        for i in range(self.len):\n","            y1, _ = getattr(self, f\"unet{i}\")(inputs)\n","            Y1.append(y1.unsqueeze(1))\n","        \n","        y1 = torch.cat(Y1, axis=1)\n","        y1 = self.fuse(y1)\n","        y2 = inputs.unsqueeze(1) - y1\n","        \n","        return y1.squeeze(1), y2.squeeze(1)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"nXub2ML_E1TO"},"source":["## Boucle d'entraînement"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = MultiUnet([3, 5]).to(device)\n","\n","loss_history = []"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/plain":["===============================================================================================\n","Layer (type:depth-idx)                        Output Shape              Param #\n","===============================================================================================\n","MultiUnet                                     [64, 6000]                --\n","├─Unet: 1-1                                   [64, 6000]                --\n","│    └─encoder_block: 2-1                     [64, 1, 6000]             --\n","│    │    └─conv_block: 3-1                   [64, 16, 6000]            912\n","│    │    └─MaxPool1d: 3-2                    [64, 16, 3000]            --\n","│    └─encoder_block: 2-2                     [64, 16, 3000]            --\n","│    │    └─conv_block: 3-3                   [64, 32, 3000]            4,800\n","│    │    └─MaxPool1d: 3-4                    [64, 32, 1500]            --\n","│    └─encoder_block: 2-3                     [64, 32, 1500]            --\n","│    │    └─conv_block: 3-5                   [64, 64, 1500]            18,816\n","│    │    └─MaxPool1d: 3-6                    [64, 64, 750]             --\n","│    └─encoder_block: 2-4                     [64, 64, 750]             --\n","│    │    └─conv_block: 3-7                   [64, 64, 750]             24,960\n","│    │    └─MaxPool1d: 3-8                    [64, 64, 375]             --\n","│    └─conv_block: 2-5                        [64, 128, 375]            --\n","│    │    └─Sequential: 3-9                   [64, 128, 375]            74,496\n","│    └─decoder_block: 2-6                     [64, 64, 750]             --\n","│    │    └─ConvTranspose1d: 3-10             [64, 64, 750]             16,448\n","│    │    └─conv_block: 3-11                  [64, 64, 750]             37,248\n","│    └─decoder_block: 2-7                     [64, 32, 1500]            --\n","│    │    └─ConvTranspose1d: 3-12             [64, 32, 1500]            4,128\n","│    │    └─conv_block: 3-13                  [64, 32, 1500]            9,408\n","│    └─decoder_block: 2-8                     [64, 16, 3000]            --\n","│    │    └─ConvTranspose1d: 3-14             [64, 16, 3000]            1,040\n","│    │    └─conv_block: 3-15                  [64, 16, 3000]            2,400\n","│    └─decoder_block: 2-9                     [64, 8, 6000]             --\n","│    │    └─ConvTranspose1d: 3-16             [64, 8, 6000]             264\n","│    │    └─conv_block: 3-17                  [64, 8, 6000]             456\n","│    └─Conv1d: 2-10                           [64, 1, 6000]             9\n","├─Unet: 1-2                                   [64, 6000]                --\n","│    └─encoder_block: 2-11                    [64, 1, 6000]             --\n","│    │    └─conv_block: 3-18                  [64, 16, 6000]            1,456\n","│    │    └─MaxPool1d: 3-19                   [64, 16, 3000]            --\n","│    └─encoder_block: 2-12                    [64, 16, 3000]            --\n","│    │    └─conv_block: 3-20                  [64, 32, 3000]            7,872\n","│    │    └─MaxPool1d: 3-21                   [64, 32, 1500]            --\n","│    └─encoder_block: 2-13                    [64, 32, 1500]            --\n","│    │    └─conv_block: 3-22                  [64, 64, 1500]            31,104\n","│    │    └─MaxPool1d: 3-23                   [64, 64, 750]             --\n","│    └─encoder_block: 2-14                    [64, 64, 750]             --\n","│    │    └─conv_block: 3-24                  [64, 64, 750]             41,344\n","│    │    └─MaxPool1d: 3-25                   [64, 64, 375]             --\n","│    └─conv_block: 2-15                       [64, 128, 375]            --\n","│    │    └─Sequential: 3-26                  [64, 128, 375]            123,648\n","│    └─decoder_block: 2-16                    [64, 64, 750]             --\n","│    │    └─ConvTranspose1d: 3-27             [64, 64, 750]             16,448\n","│    │    └─conv_block: 3-28                  [64, 64, 750]             61,824\n","│    └─decoder_block: 2-17                    [64, 32, 1500]            --\n","│    │    └─ConvTranspose1d: 3-29             [64, 32, 1500]            4,128\n","│    │    └─conv_block: 3-30                  [64, 32, 1500]            15,552\n","│    └─decoder_block: 2-18                    [64, 16, 3000]            --\n","│    │    └─ConvTranspose1d: 3-31             [64, 16, 3000]            1,040\n","│    │    └─conv_block: 3-32                  [64, 16, 3000]            3,936\n","│    └─decoder_block: 2-19                    [64, 8, 6000]             --\n","│    │    └─ConvTranspose1d: 3-33             [64, 8, 6000]             264\n","│    │    └─conv_block: 3-34                  [64, 8, 6000]             728\n","│    └─Conv1d: 2-20                           [64, 1, 6000]             9\n","├─Conv1d: 1-3                                 [64, 1, 6000]             3\n","├─Conv1d: 1-4                                 [64, 1, 6000]             (recursive)\n","===============================================================================================\n","Total params: 504,741\n","Trainable params: 504,741\n","Non-trainable params: 0\n","Total mult-adds (G): 27.58\n","===============================================================================================\n","Input size (MB): 1.54\n","Forward/backward pass size (MB): 2568.19\n","Params size (MB): 2.02\n","Estimated Total Size (MB): 2571.75\n","==============================================================================================="]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["torchinfo.summary(model, X_train_torch[0].shape)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["lr = 1e-4\n","loss_fn = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6HeK1_UE1TO","outputId":"e3c3e644-bf70-44ee-92d3-979a57324a1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 2/128 [00:03<03:53,  1.85s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m Y1 \u001b[38;5;241m=\u001b[39m Y1_train_torch[i]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m Y2 \u001b[38;5;241m=\u001b[39m Y2_train_torch[i]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m Y1_pred, Y2_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(loss_fn(Y1_pred, Y1) \u001b[38;5;241m+\u001b[39m loss_fn(Y2_pred, Y2), loss_fn(Y1_pred, Y2) \u001b[38;5;241m+\u001b[39m loss_fn(Y2_pred, Y1))\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[9], line 80\u001b[0m, in \u001b[0;36mMultiUnet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     78\u001b[0m Y1 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlen):\n\u001b[0;32m---> 80\u001b[0m     y1, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munet\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     Y1\u001b[38;5;241m.\u001b[39mappend(y1\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     83\u001b[0m y1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(Y1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[9], line 55\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     53\u001b[0m s1, p1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me1(inputs)\n\u001b[1;32m     54\u001b[0m s2, p2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2(p1)\n\u001b[0;32m---> 55\u001b[0m s3, p3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m s4, p4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me4(p3)\n\u001b[1;32m     57\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb(p4)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mencoder_block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     22\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[0;32m---> 23\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, p\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/pooling.py:92\u001b[0m, in \u001b[0;36mMaxPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:696\u001b[0m, in \u001b[0;36m_max_pool1d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["epochs = 10\n","model.train()\n","for e in range(epochs):\n","    print(f\"Epoch {e+1}/{epochs}\")\n","    for i in tqdm(range(len(X_train_torch))):\n","        optimizer.zero_grad()\n","        X = X_train_torch[i].to(device)\n","        Y1 = Y1_train_torch[i].to(device)\n","        Y2 = Y2_train_torch[i].to(device)\n","\n","        Y1_pred, Y2_pred = model(X)\n","        loss = torch.min(loss_fn(Y1_pred, Y1) + loss_fn(Y2_pred, Y2), loss_fn(Y1_pred, Y2) + loss_fn(Y2_pred, Y1))\n","\n","        loss.backward()\n","        optimizer.step()\n","        loss_history.append(loss.item())\n","    scheduler.step()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(np.log(loss_history), label=\"loss\")"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"i5BMfkzUE1TP"},"source":["## Prédiction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RCPOhiHE1TP","outputId":"1a885526-e340-4b95-fd48-8fca840ffeb7"},"outputs":[],"source":["model.eval()\n","\n","predictions = np.array([])\n","predictions = predictions.reshape(0, 2, 6000)\n","\n","for i in range(len(X_test_torch)):\n","    X = X_test_torch[i].to(device)\n","    with torch.no_grad():\n","        Y1_pred, Y2_pred = model(X)\n","        Y_pred = torch.stack([Y1_pred, Y2_pred], dim=1)\n","    predictions = np.concatenate([predictions, Y_pred.cpu().numpy()])\n","\n","np.save(\"predictions.npy\", predictions)\n","!zip predictions.zip predictions.npy"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
