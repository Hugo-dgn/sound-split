{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"xBvdio9IE1TB"},"source":["## Import des libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFOfNAFFE1TG"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import os\n","from scipy.io import wavfile\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","import torchinfo"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"8DDcCG-BE1TH"},"source":["## Téléchargement du dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRl7xvE1E1TH"},"outputs":[],"source":["!gdown 1Q3sbordKpx65ExOvqtM4fif7rDXhIXwS\n","!unzip datasetaudio.zip"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"eJxJ2WZtE1TI"},"source":["Sinon, le lien est ici: https://drive.google.com/file/d/1Q3sbordKpx65ExOvqtM4fif7rDXhIXwS/view?usp=sharing"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"cPyxcartE1TI"},"source":["## Load le dataset\n","\n","### Load le dataset de train\n","Pour load un fichier .wav, on utilise la fonction `wavfile.read()` de `scipy.io` qui nous retourne un tuple `(samplerate, data)`. `data` est un array numpy de dimension 1, contenant les valeurs du signal audio. Le nombre d'échantillons par seconde pour nos données est de 4000, donc chaque fichier .wav contient 6000 valeurs car la durée du signal est de 1.5 secondes. `data` est donc de shape `(6000,)`.\n","\n","Il y a 4096 fichiers .wav dans le dossier `train/x_train` et 2048 fichiers .wav dans le dossier `test/x_test`. Donc on peut créer un array numpy de shape `(4096, 6000)` pour les données d'entraînement et un array numpy de shape `(2048, 6000)` pour les données de test."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3Yk5YxmE1TI","outputId":"4b8c6bef-e4b4-4f93-aa36-ac7a5a037661"},"outputs":[],"source":["X_train = []\n","Y1_train = []\n","Y2_train = []\n","\n","N_TRAIN = 4096\n","\n","for i in range(N_TRAIN):\n","    x = wavfile.read(f\"train/x_train/{i}.wav\")[1]\n","    y1 = wavfile.read(f\"train/y_train/{i}-a.wav\")[1]\n","    y2 = wavfile.read(f\"train/y_train/{i}-b.wav\")[1]\n","\n","    X_train.append(x)\n","    Y1_train.append(y1)\n","    Y2_train.append(y2)\n","\n","X_train = np.array(X_train)\n","Y1_train = np.array(Y1_train)\n","Y2_train = np.array(Y2_train)\n","\n","print(X_train.shape)\n","print(Y1_train.shape)\n","print(Y2_train.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Data augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fact = 2**1\n","aug = (fact-1)*N_TRAIN\n","\n","#get aug pairs of random indices\n","idx = np.random.randint(0, N_TRAIN, size=(aug, 2))\n","#sum the signals\n","Y1_train_aug = Y1_train[idx[:,0]]\n","Y2_train_aug = Y2_train[idx[:,1]]\n","X_train_aug = Y1_train_aug + Y2_train_aug\n","X_train_aug.shape\n","\n","X_train = np.concatenate([X_train, X_train_aug])\n","Y1_train = np.concatenate([Y1_train, Y1_train_aug])\n","Y2_train = np.concatenate([Y2_train, Y2_train_aug])\n","\n","print(X_train.shape)\n","print(Y1_train.shape)\n","print(Y2_train.shape)\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"xUSBhIGRE1TJ"},"source":["### Load le dataset de test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aV-kx9CJE1TK","outputId":"f9227049-c632-4b07-8e49-c73c7fba3aa4"},"outputs":[],"source":["X_test = []\n","\n","N_TEST = 512\n","\n","for i in range(N_TEST):\n","    x = wavfile.read(f\"test/x_test/{i}.wav\")[1]\n","    X_test.append(x)\n","\n","X_test = np.array(X_test)\n","\n","print(X_test.shape)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"clNGtVtnE1TK"},"source":["## Lecture des données"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MyHkXzuIE1TK"},"outputs":[],"source":["import IPython.display as ipd\n","\n","SAMPLERATE = 4000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"YQI0VeROE1TK","outputId":"ebff1e2b-dc7a-46c1-f7c1-4e23db3d68ed"},"outputs":[],"source":["\n","ipd.Audio(X_train[0], rate=SAMPLERATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"d-w8hR-ME1TL","outputId":"f39bc228-fec4-4500-e334-84e8c6622bf1"},"outputs":[],"source":["ipd.Audio(Y1_train[0], rate=SAMPLERATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"WPstIGhnE1TL","outputId":"31448f45-cb5b-4a1d-8d6c-bc6043ebaa4b"},"outputs":[],"source":["ipd.Audio(Y2_train[0], rate=SAMPLERATE)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"GeEixPgNE1TM"},"source":["## Batch les données"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQI0c5SuE1TM","outputId":"8e98889e-963b-487c-d189-e6bed58dea93"},"outputs":[],"source":["batch_size = 64\n","X_train_reshaped = X_train.reshape(-1, batch_size, 6000)\n","Y1_train_reshaped = Y1_train.reshape(-1, batch_size, 6000)\n","Y2_train_reshaped = Y2_train.reshape(-1, batch_size, 6000)\n","\n","print(X_train_reshaped.shape)\n","print(Y1_train_reshaped.shape)\n","print(Y2_train_reshaped.shape)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"XnAElbtRE1TM"},"source":["## Convertir les données en torch.tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bdbjUkWE1TN"},"outputs":[],"source":["X_train_torch = torch.from_numpy(X_train_reshaped).float()\n","Y1_train_torch = torch.from_numpy(Y1_train_reshaped).float()\n","Y2_train_torch = torch.from_numpy(Y2_train_reshaped).float()"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"1FNjFObBE1TN"},"source":["## Faire la même chose pour le dataset de test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HynKreAE1TN","outputId":"6ff3b654-6b7d-4f6c-a79d-182b50d89489"},"outputs":[],"source":["X_test_reshaped = X_test.reshape(-1, batch_size, 6000)\n","X_test_torch = torch.from_numpy(X_test_reshaped).float()\n","\n","print(X_test_torch.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Netoyage"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#delete unsuse arrays\n","del X_train\n","del Y1_train\n","del Y2_train\n","del X_test\n","del X_train_reshaped\n","del Y1_train_reshaped\n","del Y2_train_reshaped\n","del X_test_reshaped"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"dditrgIFE1TO"},"source":["## Créer le modèle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHo_cqBxE1TO"},"outputs":[],"source":["kernel_size = 51\n","\n","class Conv1D(nn.Module):\n","    def __init__(self):\n","        super(Conv1D, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(32),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(64),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(128),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(256),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(512),\n","            nn.PReLU()\n","        )\n","        \n","        self.decoder = nn.Sequential(\n","            nn.Conv1d(in_channels=512, out_channels=256, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(256),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=256, out_channels=128, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(128),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(64),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=64, out_channels=32, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","            nn.BatchNorm1d(32),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=32, out_channels=1, kernel_size=kernel_size, stride=1, padding=\"same\"),\n","        )\n","        \n","    def forward(self, x):\n","        x = x.unsqueeze(1)\n","        x1 = self.encoder(x)\n","        x1 = self.decoder(x1)\n","        x2 = x - x1\n","        \n","        return x1.squeeze(1), x2.squeeze(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kernel_size = 51\n","\n","class conv_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","        self.network = nn.Sequential(\n","            nn.Conv1d(in_c, out_c, kernel_size=kernel_size, padding=\"same\"),\n","            nn.BatchNorm1d(out_c),\n","            nn.ReLU(),\n","            nn.Conv1d(out_c, out_c, kernel_size=kernel_size, padding=\"same\"),\n","            nn.BatchNorm1d(out_c),\n","            nn.ReLU()\n","        )\n","    def forward(self, x):\n","        y = self.network(x)\n","        return y\n","\n","class encoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","        self.conv = conv_block(in_c, out_c)\n","        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n","    def forward(self, x):\n","        y = self.conv(x)\n","        p = self.pool(y)\n","        return x, p\n","\n","class decoder_block(nn.Module):\n","    def __init__(self, in_c, out_c, in_skip):\n","        super().__init__()\n","        self.up = nn.ConvTranspose1d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n","        self.conv = conv_block(out_c+in_skip, out_c)\n","    def forward(self, inputs, skip):\n","        x = self.up(inputs)\n","        x = torch.cat([x, skip], axis=1)\n","        x = self.conv(x)\n","        return x\n","\n","class Unet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.e1 = encoder_block(1, 16)\n","        self.e2 = encoder_block(16, 32)\n","        self.e3 = encoder_block(32, 64)\n","        self.e4 = encoder_block(64, 128)\n","        self.b = conv_block(128, 128)\n","        self.d1 = decoder_block(128, 64, 64)\n","        self.d2 = decoder_block(64, 32, 32)\n","        self.d3 = decoder_block(32, 16, 16)\n","        self.d4 = decoder_block(16, 8, 1)\n","        self.outputs = nn.Conv1d(8, 1, kernel_size=1, padding=0)\n","        \n","    def forward(self, inputs):\n","        inputs = inputs.unsqueeze(1)\n","        s1, p1 = self.e1(inputs)\n","        s2, p2 = self.e2(p1)\n","        s3, p3 = self.e3(p2)\n","        s4, p4 = self.e4(p3)\n","        b = self.b(p4)\n","        d1 = self.d1(b, s4)\n","        d2 = self.d2(d1, s3)\n","        d3 = self.d3(d2, s2)\n","        d4 = self.d4(d3, s1)\n","        outputs = self.outputs(d4)\n","        \n","        y1 = outputs\n","        y2 = inputs - outputs\n","        return y1.squeeze(1), y2.squeeze(1)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"nXub2ML_E1TO"},"source":["## Boucle d'entraînement"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Unet().to(device)\n","\n","loss_history = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torchinfo.summary(model, X_train_torch[0].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lr = 1e-4\n","loss_fn = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6HeK1_UE1TO","outputId":"e3c3e644-bf70-44ee-92d3-979a57324a1f"},"outputs":[],"source":["epochs = 10\n","model.train()\n","for e in range(epochs):\n","    print(f\"Epoch {e+1}/{epochs}\")\n","    for i in tqdm(range(len(X_train_torch))):\n","        optimizer.zero_grad()\n","        X = X_train_torch[i].to(device)\n","        Y1 = Y1_train_torch[i].to(device)\n","        Y2 = Y2_train_torch[i].to(device)\n","\n","        Y1_pred, Y2_pred = model(X)\n","        loss = torch.min(loss_fn(Y1_pred, Y1) + loss_fn(Y2_pred, Y2), loss_fn(Y1_pred, Y2) + loss_fn(Y2_pred, Y1))\n","\n","        loss.backward()\n","        optimizer.step()\n","        loss_history.append(loss.item())\n","    scheduler.step()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(np.log(loss_history), label=\"loss\")"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"i5BMfkzUE1TP"},"source":["## Prédiction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RCPOhiHE1TP","outputId":"1a885526-e340-4b95-fd48-8fca840ffeb7"},"outputs":[],"source":["model.eval()\n","\n","predictions = np.array([])\n","predictions = predictions.reshape(0, 2, 6000)\n","\n","for i in range(len(X_test_torch)):\n","    X = X_test_torch[i].to(device)\n","    with torch.no_grad():\n","        Y1_pred, Y2_pred = model(X)\n","        Y_pred = torch.stack([Y1_pred, Y2_pred], dim=1)\n","    predictions = np.concatenate([predictions, Y_pred.cpu().numpy()])\n","\n","np.save(\"predictions.npy\", predictions)\n","!zip predictions.zip predictions.npy"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
