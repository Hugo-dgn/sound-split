{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"xBvdio9IE1TB"},"source":["## Import des libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"aFOfNAFFE1TG"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/hugo/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import os\n","from scipy.io import wavfile\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from torchmetrics.audio import PermutationInvariantTraining\n","from torchmetrics.functional.audio import scale_invariant_signal_noise_ratio"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"8DDcCG-BE1TH"},"source":["## Téléchargement du dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRl7xvE1E1TH"},"outputs":[],"source":["!gdown 1Q3sbordKpx65ExOvqtM4fif7rDXhIXwS\n","!unzip datasetaudio.zip"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"eJxJ2WZtE1TI"},"source":["Sinon, le lien est ici: https://drive.google.com/file/d/1Q3sbordKpx65ExOvqtM4fif7rDXhIXwS/view?usp=sharing"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"cPyxcartE1TI"},"source":["## Load le dataset\n","\n","### Load le dataset de train\n","Pour load un fichier .wav, on utilise la fonction `wavfile.read()` de `scipy.io` qui nous retourne un tuple `(samplerate, data)`. `data` est un array numpy de dimension 1, contenant les valeurs du signal audio. Le nombre d'échantillons par seconde pour nos données est de 4000, donc chaque fichier .wav contient 6000 valeurs car la durée du signal est de 1.5 secondes. `data` est donc de shape `(6000,)`.\n","\n","Il y a 4096 fichiers .wav dans le dossier `train/x_train` et 2048 fichiers .wav dans le dossier `test/x_test`. Donc on peut créer un array numpy de shape `(4096, 6000)` pour les données d'entraînement et un array numpy de shape `(2048, 6000)` pour les données de test."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3Yk5YxmE1TI","outputId":"4b8c6bef-e4b4-4f93-aa36-ac7a5a037661"},"outputs":[{"name":"stdout","output_type":"stream","text":["(4096, 6000)\n","(4096, 6000)\n","(4096, 6000)\n"]}],"source":["X_train = []\n","Y1_train = []\n","Y2_train = []\n","\n","N_TRAIN = 4096\n","\n","for i in range(N_TRAIN):\n","    x = wavfile.read(f\"train/x_train/{i}.wav\")[1]\n","    y1 = wavfile.read(f\"train/y_train/{i}-a.wav\")[1]\n","    y2 = wavfile.read(f\"train/y_train/{i}-b.wav\")[1]\n","\n","    X_train.append(x)\n","    Y1_train.append(y1)\n","    Y2_train.append(y2)\n","\n","X_train = np.array(X_train)\n","Y1_train = np.array(Y1_train)\n","Y2_train = np.array(Y2_train)\n","\n","print(X_train.shape)\n","print(Y1_train.shape)\n","print(Y2_train.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Data augmentation"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(8192, 6000)\n","(8192, 6000)\n","(8192, 6000)\n"]}],"source":["fact = 2**1\n","aug = (fact-1)*N_TRAIN\n","\n","#get aug pairs of random indices\n","idx = np.random.randint(0, N_TRAIN, size=(aug, 2))\n","#sum the signals\n","Y1_train_aug = Y1_train[idx[:,0]]\n","Y2_train_aug = Y2_train[idx[:,1]]\n","X_train_aug = Y1_train_aug + Y2_train_aug\n","X_train_aug.shape\n","\n","X_train = np.concatenate([X_train, X_train_aug])\n","Y1_train = np.concatenate([Y1_train, Y1_train_aug])\n","Y2_train = np.concatenate([Y2_train, Y2_train_aug])\n","\n","print(X_train.shape)\n","print(Y1_train.shape)\n","print(Y2_train.shape)\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"xUSBhIGRE1TJ"},"source":["### Load le dataset de test"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aV-kx9CJE1TK","outputId":"f9227049-c632-4b07-8e49-c73c7fba3aa4"},"outputs":[{"name":"stdout","output_type":"stream","text":["(512, 6000)\n"]}],"source":["X_test = []\n","\n","N_TEST = 512\n","\n","for i in range(N_TEST):\n","    x = wavfile.read(f\"test/x_test/{i}.wav\")[1]\n","    X_test.append(x)\n","\n","X_test = np.array(X_test)\n","\n","print(X_test.shape)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"clNGtVtnE1TK"},"source":["## Lecture des données"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"MyHkXzuIE1TK"},"outputs":[],"source":["import IPython.display as ipd\n","\n","SAMPLERATE = 4000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"YQI0VeROE1TK","outputId":"ebff1e2b-dc7a-46c1-f7c1-4e23db3d68ed"},"outputs":[],"source":["\n","ipd.Audio(X_train[0], rate=SAMPLERATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"d-w8hR-ME1TL","outputId":"f39bc228-fec4-4500-e334-84e8c6622bf1"},"outputs":[],"source":["ipd.Audio(Y1_train[0], rate=SAMPLERATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"WPstIGhnE1TL","outputId":"31448f45-cb5b-4a1d-8d6c-bc6043ebaa4b"},"outputs":[],"source":["ipd.Audio(Y2_train[0], rate=SAMPLERATE)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"GeEixPgNE1TM"},"source":["## Batch les données"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQI0c5SuE1TM","outputId":"8e98889e-963b-487c-d189-e6bed58dea93"},"outputs":[{"name":"stdout","output_type":"stream","text":["(128, 64, 6000)\n","(128, 64, 6000)\n","(128, 64, 6000)\n"]}],"source":["batch_size = 64\n","X_train_reshaped = X_train.reshape(-1, batch_size, 6000)\n","Y1_train_reshaped = Y1_train.reshape(-1, batch_size, 6000)\n","Y2_train_reshaped = Y2_train.reshape(-1, batch_size, 6000)\n","\n","print(X_train_reshaped.shape)\n","print(Y1_train_reshaped.shape)\n","print(Y2_train_reshaped.shape)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"XnAElbtRE1TM"},"source":["## Convertir les données en torch.tensor"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"9bdbjUkWE1TN"},"outputs":[],"source":["X_train_torch = torch.from_numpy(X_train_reshaped).float()\n","Y1_train_torch = torch.from_numpy(Y1_train_reshaped).float()\n","Y2_train_torch = torch.from_numpy(Y2_train_reshaped).float()"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"1FNjFObBE1TN"},"source":["## Faire la même chose pour le dataset de test"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HynKreAE1TN","outputId":"6ff3b654-6b7d-4f6c-a79d-182b50d89489"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 64, 6000])\n"]}],"source":["X_test_reshaped = X_test.reshape(-1, batch_size, 6000)\n","X_test_torch = torch.from_numpy(X_test_reshaped).float()\n","\n","print(X_test_torch.shape)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"dditrgIFE1TO"},"source":["## Créer le modèle"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"tHo_cqBxE1TO"},"outputs":[],"source":["\n","class Conv1D(nn.Module):\n","    def __init__(self):\n","        super(Conv1D, self).__init__()\n","        \n","        self.encoder = nn.Sequential(\n","            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm1d(64),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm1d(128),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm1d(256),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm1d(512),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm1d(1024),\n","            nn.PReLU()\n","        )\n","        \n","        self.decoder = nn.Sequential(\n","            nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm1d(512),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm1d(256),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm1d(128),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm1d(64),\n","            nn.PReLU(),\n","            nn.Conv1d(in_channels=64, out_channels=1, kernel_size=3, stride=1, padding=1),\n","        )\n","        \n","    def forward(self, x):\n","        x = x.unsqueeze(1)\n","        x1 = self.encoder(x)\n","        x1 = self.decoder(x1)\n","        x2 = x - x1\n","        \n","        return x1.squeeze(1), x2.squeeze(1)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["class conv_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","        self.network = nn.Sequential(\n","            nn.Conv1d(in_c, out_c, kernel_size=3, padding=1),\n","            nn.BatchNorm1d(out_c),\n","            nn.ReLU(),\n","            nn.Conv1d(out_c, out_c, kernel_size=3, padding=1),\n","            nn.BatchNorm1d(out_c),\n","            nn.ReLU()\n","        )\n","    def forward(self, x):\n","        y = self.network(x)\n","        return y\n","\n","class encoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","        self.conv = conv_block(in_c, out_c)\n","        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n","    def forward(self, x):\n","        y = self.conv(x)\n","        p = self.pool(y)\n","        return x, p\n","\n","class decoder_block(nn.Module):\n","    def __init__(self, in_c, out_c, in_skip):\n","        super().__init__()\n","        self.up = nn.ConvTranspose1d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n","        self.conv = conv_block(out_c+in_skip, out_c)\n","    def forward(self, inputs, skip):\n","        x = self.up(inputs)\n","        x = torch.cat([x, skip], axis=1)\n","        x = self.conv(x)\n","        return x\n","\n","class Unet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.e1 = encoder_block(1, 64)\n","        self.e2 = encoder_block(64, 128)\n","        self.e3 = encoder_block(128, 256)\n","        self.e4 = encoder_block(256, 512)\n","        self.b = conv_block(512, 1024)\n","        self.d1 = decoder_block(1024, 512, 256)\n","        self.d2 = decoder_block(512, 256, 128)\n","        self.d3 = decoder_block(256, 128, 64)\n","        self.d4 = decoder_block(128, 64, 1)\n","        self.outputs = nn.Conv1d(64, 1, kernel_size=1, padding=0)\n","        \n","    def forward(self, inputs):\n","        inputs = inputs.unsqueeze(1)\n","        s1, p1 = self.e1(inputs)\n","        s2, p2 = self.e2(p1)\n","        s3, p3 = self.e3(p2)\n","        s4, p4 = self.e4(p3)\n","        b = self.b(p4)\n","        d1 = self.d1(b, s4)\n","        d2 = self.d2(d1, s3)\n","        d3 = self.d3(d2, s2)\n","        d4 = self.d4(d3, s1)\n","        outputs = self.outputs(d4)\n","        \n","        y1 = outputs\n","        y2 = inputs - outputs\n","        return y1.squeeze(1), y2.squeeze(1)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"nXub2ML_E1TO"},"source":["## Boucle d'entraînement"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Unet().to(device)\n","\n","loss_history = []"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["lr = 1e-4\n","loss_fn = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6HeK1_UE1TO","outputId":"e3c3e644-bf70-44ee-92d3-979a57324a1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 1/128 [00:16<35:15, 16.66s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m Y1 \u001b[38;5;241m=\u001b[39m Y1_train_torch[i]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m Y2 \u001b[38;5;241m=\u001b[39m Y2_train_torch[i]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m Y1_pred, Y2_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(loss_fn(Y1_pred, Y1) \u001b[38;5;241m+\u001b[39m loss_fn(Y2_pred, Y2), loss_fn(Y1_pred, Y2) \u001b[38;5;241m+\u001b[39m loss_fn(Y2_pred, Y1))\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[22], line 56\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     54\u001b[0m s2, p2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2(p1)\n\u001b[1;32m     55\u001b[0m s3, p3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me3(p2)\n\u001b[0;32m---> 56\u001b[0m s4, p4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb(p4)\n\u001b[1;32m     58\u001b[0m d1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md1(b, s4)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[22], line 22\u001b[0m, in \u001b[0;36mencoder_block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(y)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, p\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[22], line 13\u001b[0m, in \u001b[0;36mconv_block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["epochs = 10\n","\n","for e in range(epochs):\n","    print(f\"Epoch {e+1}/{epochs}\")\n","    for i in tqdm(range(len(X_train_torch))):\n","        optimizer.zero_grad()\n","        X = X_train_torch[i].to(device)\n","        Y1 = Y1_train_torch[i].to(device)\n","        Y2 = Y2_train_torch[i].to(device)\n","\n","        Y1_pred, Y2_pred = model(X)\n","        loss = torch.min(loss_fn(Y1_pred, Y1) + loss_fn(Y2_pred, Y2), loss_fn(Y1_pred, Y2) + loss_fn(Y2_pred, Y1))\n","\n","        loss.backward()\n","        optimizer.step()\n","        loss_history.append(loss.item())\n","    scheduler.step()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(np.log(loss_history), label=\"loss\")"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"i5BMfkzUE1TP"},"source":["## Prédiction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RCPOhiHE1TP","outputId":"1a885526-e340-4b95-fd48-8fca840ffeb7"},"outputs":[],"source":["model.eval()\n","\n","predictions = np.array([])\n","predictions = predictions.reshape(0, 2, 6000)\n","\n","for i in range(len(X_test_torch)):\n","    X = X_test_torch[i].to(device)\n","    with torch.no_grad():\n","        Y1_pred, Y2_pred = model(X)\n","        Y_pred = torch.stack([Y1_pred, Y2_pred], dim=1)\n","    predictions = np.concatenate([predictions, Y_pred.cpu().numpy()])\n","\n","np.save(\"predictions.npy\", predictions)\n","!zip predictions.zip predictions.npy"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
